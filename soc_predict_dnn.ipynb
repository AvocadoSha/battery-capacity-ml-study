!pip install tensorflow numpy pandas scipy matplotlib
import numpy as np
import pandas as pd
import scipy.io
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LeakyReLU, InputLayer,
Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import LearningRateScheduler
import matplotlib.pyplot as plt
from google.colab import drive
# Load MATLAB files from Google Drive
drive.mount('/content/drive')
# Load training MAT file
mat_data = scipy.io.loadmat('/content/drive/My
Drive/CHEMINFOFINAL/train.mat')
# Check available keys
print("Keys in MAT file:", mat_data.keys())
# Extract input features (X) and target values (Y), assuming 'X' and 'Y'
are present
X_data = mat_data['X'].T # Transpose to match (samples, features)
Y_data = mat_data['Y'].T.flatten() # Flatten Y to ensure correct shape
# Convert to DataFrame
df = pd.DataFrame(np.hstack((X_data, Y_data.reshape(-1, 1))))
# Normalize features (zero-center)
features = df.iloc[:, :-1].values
targets = df.iloc[:, -1].values
mean = features.mean(axis=0)
std = features.std(axis=0)
features = (features - mean) / std # Standardization
# Split into training and validation sets (80% train, 20% validation)
val_split = int(0.8 * len(features))
train_features, train_targets = features[:val_split], targets[:val_split]
val_features, val_targets = features[val_split:], targets[val_split:]
# Define model parameters
num_features = train_features.shape[1]
num_responses = 1
num_hidden_neurons = 55
epochs = 1200
mini_batch_size = 1
lr_drop_period = 400
initial_lr = 0.01
lr_drop_factor = 0.1
val_frequency = 30
# Define the Deep Neural Network (DNN) model
model = Sequential([
InputLayer(input_shape=(num_features,)), # Equivalent to
sequenceInputLayer
Dense(num_hidden_neurons),
Activation('tanh'), # Equivalent to tanhLayer
Dense(num_hidden_neurons),
LeakyReLU(alpha=0.3), # Equivalent to leakyReluLayer(0.3)
Dense(num_responses),
Activation('relu'), # Equivalent to clippedReLU (clipping can be
customized)
])
# Define piecewise learning rate schedule
def scheduler(epoch, lr):
if epoch in [400, 800]: # Reduce at epochs 400 and 800
return lr * lr_drop_factor
return lr
lr_scheduler = LearningRateScheduler(scheduler)
# Compile the model
model.compile(optimizer=Adam(learning_rate=initial_lr), loss='mse',
metrics=['mse'])
# Train the model
history = model.fit(
train_features, train_targets,
epochs=epochs,
batch_size=mini_batch_size,
validation_data=(val_features, val_targets),
callbacks=[lr_scheduler],
verbose=1 # Set to 0 for silent training
)
# Plot training loss and validation loss
plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss',
linestyle='dashed')
plt.xlabel("Epochs")
plt.ylabel("Loss (MSE)")
plt.title("Training Progress of the Deep Neural Network")
plt.legend()
plt.show()
# Define test MAT file paths
test_files = {
"n10degC": "/content/drive/My
Drive/CHEMINFOFINAL/01_TEST_LGHG2@n10degC_Norm_(05_Inputs).mat",
"0degC": "/content/drive/My
Drive/CHEMINFOFINAL/02_TEST_LGHG2@0degC_Norm_(05_Inputs).mat",
"10degC": "/content/drive/My
Drive/CHEMINFOFINAL/03_TEST_LGHG2@10degC_Norm_(05_Inputs).mat",
"25degC": "/content/drive/My
Drive/CHEMINFOFINAL/04_TEST_LGHG2@25degC_Norm_(05_Inputs).mat"
}
# Store predictions and actual SOC values
predictions = {}
targets = {}
# Make predictions for each temperature condition
for temp, file in test_files.items():
# Load MAT file
mat_data = scipy.io.loadmat(file)
# Extract X and Y data
X_test = mat_data['X'].T # Assuming features are stored in 'X',
transpose if needed
Y_test = mat_data['Y'].T.flatten() # Assuming target SOC values are
stored in 'Y'
# Predict SOC
Y_pred = model.predict(X_test, batch_size=1)
# Store results
predictions[temp] = Y_pred.flatten()
targets[temp] = Y_test
# Create SOC Comparison Plot
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
# Temperature Labels
temps = ["n10degC", "0degC", "10degC", "25degC"]
# Plot each SOC curve
for i, ax in enumerate(axes.flat):
temp = temps[i]
ax.plot(predictions[temp], label="Predicted", color="blue")
ax.plot(targets[temp], label="Target", color="orange")
ax.set_xlabel("Time (s)")
ax.set_ylabel("SOC")
ax.set_title(temp)
ax.legend()
plt.tight_layout()
plt.show()
# Compute RMSE and MAX Errors
rmse_values = []
max_errors = []
for temp in temps:
rmse = np.sqrt(mean_squared_error(targets[temp], predictions[temp])) *
100
max_error = np.max(np.abs(targets[temp] - predictions[temp])) * 100
rmse_values.append(rmse)
max_errors.append(max_error)
# Plot RMSE and MAX Errors
fig, axes = plt.subplots(2, 1, figsize=(10, 8))
axes[0].bar(temps, rmse_values, color="blue")
axes[0].set_ylabel("RMSE (%)")
axes[0].set_xlabel("Temperature (C)")
axes[1].bar(temps, max_errors, color="red")
axes[1].set_ylabel("MAX (%)")
axes[1].set_xlabel("Temperature (C)")
plt.tight_layout()
plt.show()
